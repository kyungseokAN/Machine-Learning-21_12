{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.5 SVM\n",
    "결정 경계선을 hyperplane이라 부르고, 결정 경계선과 가장 가까운 벡터를 SV(Support Vector)라 부른다. 이때, 경계선과 SV간의 거리를 마진이라하고, 이 <U>마진을 최대화하는 hyperplane을 찾는 것이 바로 SVM이 하는 일이다.</U>\n",
    "\n",
    ">참고:\n",
    "- 라그란지안 dual: https://youtu.be/O6Ha_XyA9ys\n",
    "- 선형 SVM: https://www.youtube.com/watch?v=qFg8cDnqYCI\n",
    "- 비선형 SVM: https://www.youtube.com/watch?v=ltjhyLkHMls\n",
    "\n",
    "이 마진을 최대화하는 hyperplane을 찾는 것은 SV만 결정되면 결정 경계선은 바로 찾아진다.\n",
    "![](https://ichi.pro/assets/images/max/724/1*NIyB9TnNuW0PTB3GxvtvJQ.jpeg)\n",
    "\n",
    "마진은 $\\cfrac 2 {|w|}$ 이기 때문에 우리는 |w|을 최소화하는 방법으로 결정 경계선 $x \\cdot w + b = 0$을 구한다.\n",
    "\n",
    "그런데 많은 경우 우리는 직선으로 데이터를 완벽히 분리하지 못하는 문제를 맞닦드리고, 이때 제대로 분리하지 못하는 데이터를 slack이라 부르는데, \"slack을 얼마나 허용할 것인가\" 가 중요한 하이퍼파라미터(C)이다. \n",
    "\n",
    "이때, loss함수는 다음과 같이 기술된다.\n",
    "$$Loss(w) = \\cfrac 1 2 |w|^2 + C*\\sum_i \\xi_i $$\n",
    "- $\\xi$ : slack 벡터의 결정 경계선과의 거리\n",
    "- C : slack에 대한 loss의 가중치\n",
    "- C가 커지면 slack을 줄이도록 하므로, 마진이 줄고, |w|가 커진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마진 = SV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
